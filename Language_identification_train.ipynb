{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language_identification_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Q5dxGdp3Fm",
        "outputId": "ef1f36ff-c77e-4c7a-80d7-35fe84abfe63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://zenodo.org/record/841984/files/wili-2018.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-23 15:03:43--  https://zenodo.org/record/841984/files/wili-2018.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.95.95\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.95.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62403646 (60M) [application/octet-stream]\n",
            "Saving to: ‘wili-2018.zip’\n",
            "\n",
            "wili-2018.zip       100%[===================>]  59.51M  11.1MB/s    in 6.0s    \n",
            "\n",
            "2019-12-23 15:03:50 (9.90 MB/s) - ‘wili-2018.zip’ saved [62403646/62403646]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH8Sd3LRrR6n"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# List .txt files in the root.\n",
        "#\n",
        "# Search query reference:\n",
        "# https://developers.google.com/drive/v2/web/search-parameters\n",
        "listed = drive.ListFile({'q': \"title contains '.txt' and 'root' in parents\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDKa9FeNrTui",
        "outputId": "ffa97690-4723-4d11-c959-324253260fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e5b0420cd036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp1IQH84rTtI"
      },
      "source": [
        "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/My Drive/foo.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvGDzy6ordNo",
        "outputId": "c53c6b74-83ea-433f-a683-7ef13f089bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!unzip wili-2018.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wili-2018.zip\n",
            "  inflating: x_train.txt             \n",
            "  inflating: y_train.txt             \n",
            "  inflating: x_test.txt              \n",
            "  inflating: y_test.txt              \n",
            "  inflating: labels.csv              \n",
            "  inflating: README.txt              \n",
            "  inflating: urls.txt                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRaTGsFBUNk",
        "outputId": "f714b515-4835-478e-ad59-fec60af39d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FJSIIP3KDDG",
        "outputId": "ff2a7613-8852-44eb-b50e-66436ff122d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF_3NKUFbd-C"
      },
      "source": [
        "with open('x_train.txt') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data_x_train = data.split('\\n')\n",
        "#print(len(data_x_train))           117501\n",
        "\n",
        "with open('x_test.txt') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data_x_test = data.split('\\n')\n",
        "#print(len(data_x_test))           117501\n",
        "\n",
        "with open('y_train.txt') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data_y_train = data.split('\\n')\n",
        "#print(len(data_y_train))           117501\n",
        "\n",
        "with open('y_test.txt') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data_y_test = data.split('\\n')\n",
        "#print(len(data_y_test))           117501"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW3kN3_tcEoW",
        "outputId": "ea6ace24-a945-47d6-d7b3-45492e389b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_x_train.pop(-1)\n",
        "data_x_test.pop(-1)\n",
        "data_y_train.pop(-1)\n",
        "data_y_test.pop(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1321fJUcPWZ",
        "outputId": "b98010a5-fca5-432b-f0ac-ca084f37209e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(len(data_x_train))\n",
        "print(len(data_x_test))\n",
        "print(len(data_y_train))\n",
        "print(len(data_y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117500\n",
            "117500\n",
            "117500\n",
            "117500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17hgtaEkdfHR"
      },
      "source": [
        "data_x_train  = pd.DataFrame(data_x_train,columns=['sentence'])\n",
        "data_y_train  = pd.DataFrame(data_y_train,columns=['language'])\n",
        "\n",
        "data_x_test   = pd.DataFrame(data_x_test,columns=['sentence'])\n",
        "data_y_test   = pd.DataFrame(data_y_test,columns=['language'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp7Ej8LjByH3"
      },
      "source": [
        "\n",
        "def process_sentence(sentence):\n",
        "    '''Removes all special characters from sentence. It will also strip out\n",
        "    extra whitespace and makes the string lowercase.\n",
        "    '''\n",
        "    return re.sub(r'[\\\\\\\\/:*«`\\'?¿\";!<>,.|()-_)(}{#$%@^&~+-=–—‘’“”„†•…′ⁿ№−、《》「」『』（），－：；]', '', sentence.lower().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH8zyO_3B-k1"
      },
      "source": [
        "x_train = data_x_train['sentence'].apply(process_sentence)\n",
        "y_train = data_y_train['language']\n",
        "\n",
        "x_test = data_x_test['sentence'].apply(process_sentence)\n",
        "y_test = data_y_test['language']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNUz1TpPwTmB"
      },
      "source": [
        "vectorizer = TfidfVectorizer(analyzer = 'char', norm = 'l2' , min_df=100, use_idf = True)\n",
        "x_train = vectorizer.fit_transform(x_train)\n",
        "x_test  = vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6eysdAZ7g9F",
        "outputId": "b6dfc398-cfa9-4a12-9a43-0b95b1e88cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(117500, 2420)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HznroNJu7g1d",
        "outputId": "56321598-bf18-4278-aef2-0fda4b26815f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(117500, 2420)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEeAYxT-uCXJ"
      },
      "source": [
        "languages = set(y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umMBp1HME0p0"
      },
      "source": [
        "def create_lookup_table(text):\n",
        "    \"\"\"Create lookup tables for vocabulary\n",
        "    :param text: The text split into words\n",
        "    :return: A tuple of dicts (text_to_int, int_to_text)\n",
        "    \"\"\"\n",
        "    \n",
        "    text_to_int = { word : i for i, word in enumerate(text)}\n",
        "    int_to_text = {   v  : k for k, v in text_to_int.items()}\n",
        "    \n",
        "    return text_to_int, int_to_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvKscGsZJVH_"
      },
      "source": [
        "languages_to_int, int_to_languages = create_lookup_table(languages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnGU6P1rKFOL"
      },
      "source": [
        "def encode_to_int(data, data_to_int):\n",
        "    \"\"\"Converts all our text to integers\n",
        "    :param data: The text to be converted\n",
        "    :return: All sentences in ints\n",
        "    \"\"\"\n",
        "    encoded_items = []\n",
        "    for language in data: \n",
        "        if language in data_to_int:\n",
        "          encoded_items.append([data_to_int[language]])\n",
        "    return encoded_items"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOScLiHtNURv",
        "outputId": "a3973fcb-1ac1-4a50-de7b-d10d446b4025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "y_train_encoded = OneHotEncoder().fit_transform(encode_to_int(y_train, languages_to_int)).toarray()\n",
        "y_test_encoded = OneHotEncoder().fit_transform(encode_to_int(y_test, languages_to_int)).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oKE-wxmlLBr"
      },
      "source": [
        "from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "from tensorflow.keras.backend import set_session\n",
        "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
        "\n",
        "config_proto = tf.ConfigProto()\n",
        "off = rewriter_config_pb2.RewriterConfig.OFF\n",
        "config_proto.graph_options.rewrite_options.arithmetic_optimization = off\n",
        "session = tf.Session(config=config_proto)\n",
        "set_session(session)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WCnQHWzCESK",
        "outputId": "48dfe87b-8c60-4fbc-db5f-33f6b829482e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=x_train.shape[1], activation='relu'))\n",
        "model.add(Dense(len(languages), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "checkpointer = ModelCheckpoint(filepath='/content/drive/My Drive/language_tf_idf_1.hdf5', monitor = \"val_loss\", verbose=1, save_best_only=True, mode = 'auto')\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               1239552   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 235)               120555    \n",
            "=================================================================\n",
            "Total params: 1,360,107\n",
            "Trainable params: 1,360,107\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8x1OomLhV2J",
        "outputId": "739fcd81-3a49-4f93-cf06-14b92c61caa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Interrupt the training after 21 epoch otherwise it will overfit (because performane on valid set is not much increasing after 12 epoch)\n",
        "score = model.fit(x_train, y_train_encoded, epochs=50, batch_size=32, validation_data=(x_test, y_test_encoded ), callbacks=[checkpointer])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117500 samples, validate on 117500 samples\n",
            "Epoch 1/50\n",
            "117472/117500 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.8624\n",
            "Epoch 00001: val_loss improved from 0.61427 to 0.52274, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 25s 214us/sample - loss: 0.5546 - acc: 0.8624 - val_loss: 0.5227 - val_acc: 0.8683\n",
            "Epoch 2/50\n",
            "117408/117500 [============================>.] - ETA: 0s - loss: 0.4821 - acc: 0.8770\n",
            "Epoch 00002: val_loss improved from 0.52274 to 0.48526, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 26s 222us/sample - loss: 0.4820 - acc: 0.8771 - val_loss: 0.4853 - val_acc: 0.8784\n",
            "Epoch 3/50\n",
            "117472/117500 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.8853\n",
            "Epoch 00003: val_loss improved from 0.48526 to 0.46980, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 26s 223us/sample - loss: 0.4451 - acc: 0.8853 - val_loss: 0.4698 - val_acc: 0.8797\n",
            "Epoch 4/50\n",
            "117184/117500 [============================>.] - ETA: 0s - loss: 0.4195 - acc: 0.8904\n",
            "Epoch 00004: val_loss improved from 0.46980 to 0.45619, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 26s 226us/sample - loss: 0.4194 - acc: 0.8904 - val_loss: 0.4562 - val_acc: 0.8843\n",
            "Epoch 5/50\n",
            "117376/117500 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8944\n",
            "Epoch 00005: val_loss improved from 0.45619 to 0.44639, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 26s 217us/sample - loss: 0.3993 - acc: 0.8944 - val_loss: 0.4464 - val_acc: 0.8865\n",
            "Epoch 6/50\n",
            "117248/117500 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8989\n",
            "Epoch 00006: val_loss improved from 0.44639 to 0.44251, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 27s 228us/sample - loss: 0.3821 - acc: 0.8988 - val_loss: 0.4425 - val_acc: 0.8868\n",
            "Epoch 7/50\n",
            "117344/117500 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.9020\n",
            "Epoch 00007: val_loss improved from 0.44251 to 0.44195, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 27s 227us/sample - loss: 0.3663 - acc: 0.9019 - val_loss: 0.4419 - val_acc: 0.8880\n",
            "Epoch 8/50\n",
            "117152/117500 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.9041\n",
            "Epoch 00008: val_loss improved from 0.44195 to 0.43616, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 26s 220us/sample - loss: 0.3530 - acc: 0.9041 - val_loss: 0.4362 - val_acc: 0.8889\n",
            "Epoch 9/50\n",
            "117376/117500 [============================>.] - ETA: 0s - loss: 0.3398 - acc: 0.9068\n",
            "Epoch 00009: val_loss improved from 0.43616 to 0.43247, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 26s 224us/sample - loss: 0.3399 - acc: 0.9068 - val_loss: 0.4325 - val_acc: 0.8895\n",
            "Epoch 10/50\n",
            "117440/117500 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.9087\n",
            "Epoch 00010: val_loss improved from 0.43247 to 0.42953, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 27s 232us/sample - loss: 0.3292 - acc: 0.9087 - val_loss: 0.4295 - val_acc: 0.8905\n",
            "Epoch 11/50\n",
            "117280/117500 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9116\n",
            "Epoch 00011: val_loss improved from 0.42953 to 0.42650, saving model to /content/drive/My Drive/language_tf_idf_1.hdf5\n",
            "117500/117500 [==============================] - 26s 222us/sample - loss: 0.3178 - acc: 0.9115 - val_loss: 0.4265 - val_acc: 0.8936\n",
            "Epoch 12/50\n",
            "117216/117500 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9143\n",
            "Epoch 00012: val_loss did not improve from 0.42650\n",
            "117500/117500 [==============================] - 26s 224us/sample - loss: 0.3065 - acc: 0.9143 - val_loss: 0.4305 - val_acc: 0.8937\n",
            "Epoch 13/50\n",
            "117152/117500 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9160\n",
            "Epoch 00013: val_loss did not improve from 0.42650\n",
            "117500/117500 [==============================] - 26s 218us/sample - loss: 0.2966 - acc: 0.9160 - val_loss: 0.4331 - val_acc: 0.8928\n",
            "Epoch 14/50\n",
            "117408/117500 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9188\n",
            "Epoch 00014: val_loss did not improve from 0.42650\n",
            "117500/117500 [==============================] - 25s 213us/sample - loss: 0.2880 - acc: 0.9188 - val_loss: 0.4291 - val_acc: 0.8945\n",
            "Epoch 15/50\n",
            "117312/117500 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9208\n",
            "Epoch 00015: val_loss did not improve from 0.42650\n",
            "117500/117500 [==============================] - 26s 224us/sample - loss: 0.2789 - acc: 0.9207 - val_loss: 0.4336 - val_acc: 0.8943\n",
            "Epoch 16/50\n",
            "117472/117500 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9222\n",
            "Epoch 00016: val_loss did not improve from 0.42650\n",
            "117500/117500 [==============================] - 26s 221us/sample - loss: 0.2708 - acc: 0.9222 - val_loss: 0.4371 - val_acc: 0.8945\n",
            "Epoch 17/50\n",
            "117344/117500 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.9246\n",
            "Epoch 00017: val_loss did not improve from 0.42650\n",
            "117500/117500 [==============================] - 26s 223us/sample - loss: 0.2628 - acc: 0.9246 - val_loss: 0.4410 - val_acc: 0.8944\n",
            "Epoch 18/50\n",
            " 27872/117500 [======>.......................] - ETA: 14s - loss: 0.2478 - acc: 0.9281"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-280-59b7dde6d9d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#for i in range(0,200):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#print(\"\\tEpoch  :  \"+str(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_encoded\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/language_model_tf_idf_1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m               \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m               \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m               \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    527\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m     return [\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    527\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m     return [\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onwf-XQC0T8f"
      },
      "source": [
        "model.save('/content/drive/My Drive/language_model_tf_idf_1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgNrTm00uQfR",
        "outputId": "3a1691f4-2b16-4088-a062-7a3977ca4390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3F6HzyuC4tf",
        "outputId": "4c9ac13a-48f8-430c-9b98-c08f06454fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "evaluate = model.evaluate(x_test, y_test_encoded, verbose=1)\n",
        "print(\"Accuracy: %.2f%%\" % (evaluate[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117500/117500 [==============================] - 8s 70us/sample - loss: 0.4454 - acc: 0.8942\n",
            "Accuracy: 89.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJx5GRCgD99h"
      },
      "source": [
        "\n",
        "predictions = model.predict(x_test)\n",
        " \n",
        "pred_list = []\n",
        "  \n",
        "actual_list = []\n",
        " \n",
        "for i in predictions:\n",
        "  pred_list.append(np.argmax(i))\n",
        " \n",
        "for i in y_test_encoded:\n",
        "  actual_list.append(np.argmax(i))\n",
        "\n",
        "y_actu = pd.Series(actual_list, name='Actual')\n",
        "y_pred = pd.Series(pred_list, name='Predicted')\n",
        "df_confusion = pd.crosstab(y_actu, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpG2nDnaEACo",
        "outputId": "314804fb-7914-4128-9bd7-0782366ca9b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(df_confusion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted  0    1    2    3    4    5    6    ...  228  229  230  231  232  233  234\n",
            "Actual                                        ...                                   \n",
            "0          469    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "1            0  334    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2            0    0  491    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "3            0    0    0  413    0    0    0  ...    1    0    0    5    0    0    0\n",
            "4            0    0    0    0  428    0    0  ...    0    0    0    1    0    0    0\n",
            "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "230          0    0    0    0    0    0    0  ...    0    0  275    0    0    0    0\n",
            "231          0    0    1    0    0    0    0  ...    0    0    0  423    0    0    0\n",
            "232          0    0    0    0    0    0    0  ...    0    0    0    0  446    1    0\n",
            "233          0    0    0    0    0    0    0  ...    0    0    0    0    0  454    0\n",
            "234          0    0    0    0    0    0    0  ...    0    0    0    0    0    0  492\n",
            "\n",
            "[235 rows x 235 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJZGVAEV0JK3"
      },
      "source": [
        "def predict_sentence(sentence):\n",
        "    \"\"\"Converts the text and sends it to the model for classification\n",
        "    :param sentence: The text to predict\n",
        "    :return: string - The language of the sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    # Clean the sentence\n",
        "    sentence = process_sentence(sentence)\n",
        "    \n",
        "    # Transform and pad it before using the model to predict\n",
        "    sentence = [sentence]\n",
        "    x = vectorizer.transform(sentence)\n",
        "        \n",
        "    prediction = model.predict(x)\n",
        "    # Get the highest prediction\n",
        "    lang_index = np.argmax(prediction)\n",
        "\n",
        "    return int_to_languages[lang_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwHpqo5yX5Ni",
        "outputId": "660032e9-39d9-4e23-9a08-6ef8831d4007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_sentence('''Ne l fin de l seclo XIX l Japon era inda çconhecido i sótico pa l mundo oucidental. \n",
        "Cula antroduçon de la stética japonesa, particularmente na Sposiçon Ounibersal de 1900, an Paris, l Oucidente \n",
        "adquiriu un apetite ansaciable pul Japon i Heiarn se tornou mundialmente coincido pula perfundidade, ouriginalidade i \n",
        "sinceridade de ls sous cuntos. An sous radadeiros anhos, alguns críticos, cumo George Orwell, acusórun Heiarn de trasferir\n",
        " sou nacionalismo i fazer l Japon parecer mais sótico, mas, cumo loufereciu al Oucidente alguns de sous purmeiros lampeijos\n",
        "  de l Japon pré-andustrial i de l Período Meiji, sou trabalho inda ye balioso até hoije.''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mwl'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYxzBB0daOBQ",
        "outputId": "cee96524-d253-469b-f6c8-894b616cace2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_sentence(\"\"\"बांग्लादेश के मुख्य न्यायाधीश का पद, बांग्लादेश सर्वोच्च न्यायिक पद है। इस पद पर विराजमान होने वाले पहले पदाधिकारी न्यायमूर्ति \n",
        "अब सादात मोहम्मह खान सयम थे, जोकि १६ दिसंबर १९७२ से नवंबर १९७५ तक इस पद पर रहे थे। तत्पश्चात, जनवरी २०१५ तक इस पद पर कुल २१ लोग विराजमान हो चुके हैं। \n",
        "वर्तमान मुख्य न्यायाधीश सुरेन्द्र कुमार सिन्हा इस पद पर १७ जनवरी २०१५ विराजमान हैं। वे हिन्दू धर्म के अनुयायी हैं, तथा बिष्णुप्रिय मणिपुरी समुदाय से आते हैं, तथा वे बांग्लादेश में\n",
        " किसी भी अल्पसंख्यक जातीय समूहों से नियुक्त पहली मुख्य न्यायाधीश है। न्यायमूर्ति भावनी प्रसाद सिन्हा भी एक ही समुदाय से हैं। न्यायमूर्ति महौदय नाज़मन आरा सुल्ताना इस पद को \n",
        " सुशोभित करने वाली पहली महिला न्यायाधीश थीं, और मैडम जस्टिस कृष्णा देबनाथ बांग्लादेश की पहली महिला हिंदू न्यायाधीश है। वर्तमान में सुप्रीम कोर्ट में छह महिला न्यायाधीशों रहे हैं।\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 420
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Fh5Md1bNp4",
        "outputId": "22683a5a-b4f3-44c0-dbcf-d1a56845d68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "confusion_matrix(actual_list, pred_list)[7][7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "493"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 376
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1EjULV8vA1t",
        "outputId": "f758f950-56ef-456b-9c1b-b09799d7c156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "int_to_languages[14]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nob'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adp8cAABsupA",
        "outputId": "5e7bea52-de9a-42a8-f4eb-484627ad6e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_sentence('''Thou art the ruler of the minds of all people,\n",
        "Dispenser of India's destiny.\n",
        "Thy name rouses the hearts of Punjab, Sindh,\n",
        " Gujarat and Maratha,\n",
        "Of the Dravida and Odisha\n",
        "and Bengal''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eng'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 421
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmBiuFnNtxgV"
      },
      "source": [
        "# Save and store vectorizer and int_to_languages to use them in other files by loading thwm\n",
        "\n",
        "with open('/content/drive/My Drive/vectorizer.pk', 'wb') as fin:\n",
        "  pickle.dump(vectorizer, fin, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/int_to_languages.pk', 'wb') as fin:\n",
        "  pickle.dump(int_to_languages, fin, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}